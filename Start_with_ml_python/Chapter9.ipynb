{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimage=cv2.imread('../input/natural-images/data/natural_images/cat/cat_0353.jpg')\nplt.imshow(image,cmap=\"gray\"),plt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given an image, you want to output a simplified version.\n\n\n*Thresholding is the process of setting pixels with intensity greater than some value to be white and less than the value to be black. A more advanced technique is adaptive thresholding, where the threshold value for a pixel is determined by the pixel intensities of its neighbors. This can be helpful when lighting conditions change over different regions in an image:\n*\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"image_grey=cv2.imread('../input/natural-images/data/natural_images/cat/cat_0353.jpg',cv2.IMREAD_GRAYSCALE)\nmax_output_value=255\nneighbourhood_size=99\nsubtract_from_mean=10\nimage_binarized=cv2.adaptiveThreshold(image_grey,max_output_value,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,neighbourhood_size,subtract_from_mean)\nplt.imshow(image_binarized,cmap=\"gray\"),plt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* max_output_value===> the maximum intensity of the output pixel intensities\n* cv2.ADAPTIVE_THRESH_GAUSSIAN_C ===> pixel’s threshold to be a weighted sum of the neighboring pixel intensities\n* The last two parameters are the block size (the size of the neighborhood used to determine a pixel’s threshold) and a constant subtracted from the calculated threshold (used to manually fine-tune the threshold). \n* THRESH_BIANRY==>A binary threshold is a simple \"either or\" threshold, where the pixels are either 255 or 0. In many cases, this would be white or black, but we have left our image colored for now, so it may be colored still. The first parameter here is the image. The next parameter is the threshold, we are choosing 10. The next is the maximum value, which we're choosing as 255. Next and finally we have the type of threshold, which we've chosen as THRESH_BINARY. Normally, a threshold of 10 would be somewhat poor of a choice. We are choosing 10, because this is a low-light picture, so we choose a low number. Normally something about 125-150 would probably work best.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"img_flower=cv2.imread('../input/natural-images/data/natural_images/flower/flower_0449.jpg')\nplt.imshow(img_flower,cmap=\"gray\"),plt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find edges of an image","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"median_intensity=np.median(img_flower)\nlower_threshold = int(max(0, (1.0 - 0.33) * median_intensity)) \nupper_threshold = int(min(255, (1.0 + 0.33) * median_intensity))\n\nimage_canny = cv2.Canny(img_flower, lower_threshold, upper_threshold)\nplt.imshow(image_canny, cmap=\"gray\"), plt.axis(\"off\") \nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DETECTING CORNERS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) \nimage_gray = np.float32(image_gray)\n\nblock_size = 2 \naperture = 29 \nfree_parameter = 0.04\n\n\ndetector_responses = cv2.cornerHarris(image_gray,block_size,  aperture, free_parameter)\n\ndetector_responses = cv2.dilate(detector_responses, None)\nthreshold = 0.02 \nimage[detector_responses > threshold * detector_responses.max()] = [255,255,255]\n\nimage_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\nplt.imshow(image_gray, cmap=\"gray\"), plt.axis(\"off\") \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\ncorners_to_detect = 10 \nminimum_quality_score = 0.05 \nminimum_distance = 25\ncorners = cv2.goodFeaturesToTrack(image_gray, corners_to_detect,  minimum_quality_score, minimum_distance)\n\ncorners = np.float32(corners)\n\nfor corner in corners:   \n    x, y = corner[0]    \n    cv2.circle(image, (x,y), 10, (255,255,255), -1)\n   \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nplt.imshow(image_rgb, cmap='gray'), plt.axis(\"off\") \nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CREATING FEATURES OG MACHINE LEARNING","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Convert image data to one-dimensional vector ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"image_10X10=cv2.resize(image_grey,(10,10))\nimage_10X10.flatten()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Images are presented as a grid of pixels. If an image is in grayscale, each pixel is presented by one value (i.e., pixel intensity: 1 if white, 0 if black). ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(image_10X10, cmap=\"gray\"), plt.axis(\"off\") \nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_color=cv2.imread('../input/natural-images/data/natural_images/cat/cat_0353.jpg',cv2.IMREAD_COLOR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If the image is in color, instead of each pixel being represented by one value, it is represented by multiple values (most often three) representing the channels (red, green, blue, etc.) that blend to make the final color of that pixel. For this reason, if our 10 × 10 image is in color, we will have 300 feature values for each observation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"image_color_10X10=cv2.resize(image_color,(10,10))\nimage_color_10X10.flatten().shape","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}