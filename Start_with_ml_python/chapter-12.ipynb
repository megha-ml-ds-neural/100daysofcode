{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Thresholding Numerical Feature Variance "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn import datasets\nfrom sklearn.feature_selection import VarianceThreshold","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features with a training-set variance lower than this threshold will be removed.  remove the features that have the same value in all samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"X=[[0,2,0,3],[0,1,4,3],[0,1,1,3]]\nselector=VarianceThreshold()\nselector.fit_transform(X)\n\n","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"array([[2, 0],\n       [1, 4],\n       [1, 1]])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#  Thresholding Binary Feature Variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\nX = [[0, 1, 0],\n     [0, 1, 1],\n     [0, 1, 0],\n     [0, 1, 1],\n     [1, 0, 0]]\nthreshold=VarianceThreshold(threshold=(.75*(1-.75)))\nthreshold.fit_transform(X)","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"array([[0],\n       [1],\n       [0],\n       [1],\n       [0]])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"In binary features (i.e. Bernoulli random variables), variance is calculated as:\nVar(x)=p(1-p)\nwhere \np\n is the proportion of observations of class 1. Therefore, by setting \np\n, we can remove features where the vast majority of observations are one class."},{"metadata":{},"cell_type":"markdown","source":"#  Handling Highly Correlated Features "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n","execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create feature matrix with two highly correlated features "},{"metadata":{"trusted":true},"cell_type":"code","source":"features=np.array([[1,1,1],[2,2,0],[3,3,1],[4,4,0],[5,5,1]])\ndataframe=pd.DataFrame(features)\ncorr_matrix=dataframe.corr().abs()\n","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\ndataframe.drop(dataframe.columns[to_drop], axis=1).head(3)\n","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"   0  2\n0  1  1\n1  2  0\n2  3  1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe.corr()","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"     0    1    2\n0  1.0  1.0  0.0\n1  1.0  1.0  0.0\n2  0.0  0.0  1.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Second, we look at the upper triangle of the correlation matrix to identify pairs of highly correlated features:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"upper","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"    0    1    2\n0 NaN  1.0  0.0\n1 NaN  NaN  0.0\n2 NaN  NaN  NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#  Removing Irrelevant Features for Classification "},{"metadata":{"trusted":true},"cell_type":"code","source":"# You have a categorical target vector \n# and want to remove uninformative features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_iris\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2,f_classif\n","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris=load_iris()\nfeatures=iris.data\ntarget=iris.target\n\nfeatures=features.astype(int)\nch2_selector=SelectKBest(chi2,k=2)\nfeatures_kBest=ch2_selector.fit_transform(features,target)\n","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Original number of features:\", features.shape[1]) \nprint(\"Reduced number of features:\", features_kBest.shape[1])\n","execution_count":5,"outputs":[{"output_type":"stream","text":"Original number of features: 4\nReduced number of features: 2\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#  Recursively Eliminating Features "},{"metadata":{},"cell_type":"markdown","source":"You want to automatically select the best features to keep. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings \nfrom sklearn.datasets import make_regression \nfrom sklearn.feature_selection import RFECV \nfrom sklearn import datasets, linear_model\n","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\nfeatures, target = make_regression(n_samples = 10000,  n_features = 100, n_informative = 2,random_state=1)\nols = linear_model.LinearRegression()\n\nrfecv = RFECV(estimator=ols, step=1, scoring=\"neg_mean_squared_error\") \nrfecv.fit(features, target) \nrfecv.transform(features)\n","execution_count":2,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}